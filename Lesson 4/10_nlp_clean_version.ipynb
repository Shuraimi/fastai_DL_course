{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shuraimi/fastai_DL_course/blob/main/10_nlp_clean_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRBPR4t4pEfi",
        "outputId": "7384ba12-2c55-45e1-e23c-4620b98f0cde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/719.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m184.3/719.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m716.8/719.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m716.8/719.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m719.8/719.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m300.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2024.5.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mMounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "#hide\n",
        "! [ -e /content ] && pip install -Uqq fastbook\n",
        "import fastbook\n",
        "fastbook.setup_book()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtIcUOE8pEfn"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "from fastbook import *\n",
        "from IPython.display import display,HTML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ez6JLWnXpEfq"
      },
      "source": [
        "# NLP Deep Dive: RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qgGBdGgpEfv"
      },
      "source": [
        "## Text Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dvLX-HOpEfx"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtO5sjS9pEfy"
      },
      "source": [
        "### Word Tokenization with fastai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below imports everything from `text.all` and then load the IMDb dataset using `untar_data`"
      ],
      "metadata": {
        "id": "cdANo_Hupc6H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "gzE4IKqgpEf0",
        "outputId": "0da58743-5c46-4bd4-f549-412baec9060d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='144441344' class='' max='144440600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [144441344/144440600 00:03&lt;00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from fastai.text.all import *\n",
        "path = untar_data(URLs.IMDB)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to `get_image_files` to get all images from a path for CV task, we use `get_text_files` to get all text files from a path and also specify the folders to restrict the search of all available docs in the path."
      ],
      "metadata": {
        "id": "ROqH0EA8qGgu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQNTuehhpEf2"
      },
      "outputs": [],
      "source": [
        "files = get_text_files(path, folders = ['train', 'test', 'unsup'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then open a file i.e. the first and then read it. We then grab the first few characters of the doc/file to view it easily."
      ],
      "metadata": {
        "id": "KixDG22kqwl9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "j6eKWtFzpEf4",
        "outputId": "35850d61-25f1-4b8e-9ad0-515157281233"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I saw the film today and was mighty impressed. The film captured the buzz i'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "txt = files[0].open().read(); txt[:75]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To tokenize the files, we use SpacyTokenizer or use WordTokenizer by fastai which refers to the default tokenizer in fastai.\n",
        "\n",
        "Then grab the first element of collection using `first` and then apply spacy.\n",
        "\n",
        "Then use coll_repr to see the n number of tokens and it also gives the total number of tokens in text."
      ],
      "metadata": {
        "id": "UXxKKpRMrWLq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJIjLs-ipEf6",
        "outputId": "7fd994a8-7319-4d26-ac1a-2ade7397d93e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(#200) ['I','saw','the','film','today','and','was','mighty','impressed','.','The','film','captured','the','buzz','in','the',\"'\",'80s','when','heavy','metal','became','the','biggest','thing','going','.','It','was'...]\n"
          ]
        }
      ],
      "source": [
        "spacy = WordTokenizer()\n",
        "toks = first(spacy([txt]))\n",
        "print(coll_repr(toks, 30))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenization rules are powerful enough that it can understand that the dot after U and S don't need to be separated."
      ],
      "metadata": {
        "id": "io6U4e4dsjZt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zjnY0GGpEf8",
        "outputId": "ad569737-c2aa-4c76-e5b6-dd1366884ce9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#9) ['The','U.S.','dollar','$','1','is','$','1.00','.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "first(spacy(['The U.S. dollar $1 is $1.00.']))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy(['The U.S. dollar $1 is $1.00.'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUwKV5mDswYO",
        "outputId": "2c411d61-f09e-4e3e-adf6-85bce49bee32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object SpacyTokenizer.__call__.<locals>.<genexpr> at 0x7e0763dc0900>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "fastai's Tokenizer has few more options and info than the WordTokenizer i.e. it adds special tokens"
      ],
      "metadata": {
        "id": "BX98BgC6tBN7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOGG4sxMpEf9",
        "outputId": "c998e2fa-4240-45fe-91eb-0b3c7a024709"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(#231) ['xxbos','i','saw','the','film','today','and','was','mighty','impressed','.','xxmaj','the','film','captured','the','buzz','in','the',\"'\",'80s','when','heavy','metal','became','the','biggest','thing','going','.','xxmaj'...]\n"
          ]
        }
      ],
      "source": [
        "tkn = Tokenizer(spacy)\n",
        "print(coll_repr(tkn(txt), 31))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get a list of all rules to see methods how the special token are added."
      ],
      "metadata": {
        "id": "9oayDYG6tRbL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-SLiBZdpEf-",
        "outputId": "66206edc-9285-482e-e309-f70b830ff036"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<function fastai.text.core.fix_html(x)>,\n",
              " <function fastai.text.core.replace_rep(t)>,\n",
              " <function fastai.text.core.replace_wrep(t)>,\n",
              " <function fastai.text.core.spec_add_spaces(t)>,\n",
              " <function fastai.text.core.rm_useless_spaces(t)>,\n",
              " <function fastai.text.core.replace_all_caps(t)>,\n",
              " <function fastai.text.core.replace_maj(t)>,\n",
              " <function fastai.text.core.lowercase(t, add_bos=True, add_eos=False)>]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "defaults.text_proc_rules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "2jmmtii3pEf_",
        "outputId": "7cb06a74-18ef-46bf-92b9-c52637d94620"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"(#11) ['xxbos','Â©','xxmaj','fast.ai','xxrep','3','w','.fast.ai','/','xxup','index']\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "coll_repr(tkn('&copy;   Fast.ai www.fast.ai/INDEX'), 31)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFmr79dBpEgA"
      },
      "source": [
        "### Subword Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We open and read the first 2000 files and convert it to a L type"
      ],
      "metadata": {
        "id": "TPOyKtg61UVv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgBmSwl1pEgA"
      },
      "outputs": [],
      "source": [
        "txts = L(o.open().read() for o in files[:2000])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(txts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6MBnbaD1gph",
        "outputId": "4cfd7dcf-c34a-4599-d2ab-608f5ec5b121"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(txts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "5f6JvSN_1mKv",
        "outputId": "f0032955-3ff1-4224-9733-3a30a990c0ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "fastcore.foundation.L"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>fastcore.foundation.L</b><br/>def __call__(cls, x=None, *args, **kwargs)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py</a>Behaves like a list of `items` but can also index with list of indices or masks</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 101);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txts[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "yNon9UvA1imj",
        "outputId": "e56c0298-73be-41d1-aebd-8f2c99725a7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I saw the film today and was mighty impressed. The film captured the buzz in the '80s when heavy metal became the biggest thing going. It was good to see Iron Maiden and Judas Priest contrasted to current bands such as Slipknot to show how the spirit of the genre is thriving. Tony Iommi, Dee Snider and Ronnie James Dio all give standout interviews with Dio's digs at Gene Simmons being especially revealing. It was also great to see '80s curiosities Accept and Quebec's own Voivod being represented. The segment contrasting the grunting, leather-clad bands such as Man O War with the lace-wearing bands such as Poison and Cinderella was unexpected and fascinating. Showing the closeted Rob Halford performing in full cruising gear for a rabid hetero audience was quite poignant. I especially like how the film stresses how the music let its fans dream, cope and find solace. Those like myself, whose interest in metal may have flagged in the intervening years would do well to view it and remind themselves what all of the fuss was about.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then define a function to perform subword tokenization and call the `setup` and pass it txts so that it can identify the most commonly occuring substrings or group of letters in words in order to tokenize.\n",
        "\n",
        "To this function, we also pass the vocab size."
      ],
      "metadata": {
        "id": "hTRpPr4j2CCv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwKOGNgupEgB"
      },
      "outputs": [],
      "source": [
        "def subword(sz):\n",
        "    sp = SubwordTokenizer(vocab_sz=sz)\n",
        "    sp.setup(txts)\n",
        "    return ' '.join(first(sp([txt]))[:40])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We call the subword function and pass 1000 as the vocab size to get the subwords of txt given."
      ],
      "metadata": {
        "id": "ymo4wbjp2yxp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZMLwqbBpEgB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "9eede246-e09b-40f7-979b-9e3e50011ef2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"â–I â–saw â–the â–film â–to day â–and â–was â–might y â–imp re s s ed . â–The â–film â–ca p t ur ed â–the â–b u z z â–in â–the â– ' 8 0 s â–when â–he a v y\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "subword(1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we choose a smaller vocab size and this will give us more tokens per sentence"
      ],
      "metadata": {
        "id": "5IE0XxS83Q1T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNdC5oldpEgC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "c598d281-3fc2-4e00-897f-862ad8bf8d5c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'â–I â– s a w â–the â–film â–to d a y â–and â–was â– m i g h t y â– i m p re s s ed . â–The â–film â–ca p t ur ed â–the â–b u z'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "subword(200)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And a larger token size will give us fewer tokens per sentence as most of the words remain as it.\n",
        "\n",
        "_ represents that the original text has space at the position."
      ],
      "metadata": {
        "id": "b9dAP_Kv3aY_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBCCddEgpEgC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "5c6d78d2-4b02-4856-b927-da44eaf4bba7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"â–I â–saw â–the â–film â–today â–and â–was â–might y â–impressed . â–The â–film â–captured â–the â–buzz â–in â–the â–'80 s â–when â–heavy â–metal â–became â–the â–bigge st â–thing â–going . â–It â–was â–good â–to â–see â–I ron â–Maid en â–and\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "subword(10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B11hOBSypEgD"
      },
      "source": [
        "\n",
        "### Numericalization with fastai\n",
        "\n",
        "There are two steps:-\n",
        "\n",
        "1. Make a list of all possible unique words(i.e. the vocab)\n",
        "2. Replace the tokens with their index in vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is tokenizing the txt(which contains a few characters of sentence/first file) using Tokenizer"
      ],
      "metadata": {
        "id": "kXruhWy-EbPs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhfmrXzHpEgD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f09c9f0-e94b-4bd3-9f13-143505fa91db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(#231) ['xxbos','i','saw','the','film','today','and','was','mighty','impressed','.','xxmaj','the','film','captured','the','buzz','in','the',\"'\",'80s','when','heavy','metal','became','the','biggest','thing','going','.','xxmaj'...]\n"
          ]
        }
      ],
      "source": [
        "toks = tkn(txt)\n",
        "print(coll_repr(tkn(txt), 31))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want to tokenize first 200 docs using the .map(tkn) for each of 200 files"
      ],
      "metadata": {
        "id": "0MeSWO71EzBz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fggDPRWrpEgE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f19c0e0-cee6-4c99-d77d-5b5d0bef0948"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#231) ['xxbos','i','saw','the','film','today','and','was','mighty','impressed'...]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "toks200 = txts[:200].map(tkn)\n",
        "toks200[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we instantiate Numericalize and call the setup method just like we did for subword tokenizer. Here we call setup so that numericalize can go through the tokens to grab unique words from it and make the vocab"
      ],
      "metadata": {
        "id": "JDCOcBAaFBJj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1iOZT3xpEgE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "5fd9ad92-0aa7-47d1-bbd9-54dc19736e2f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"(#2040) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the',',','.','and','a','to','of','is','in','it','i'...]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "num = Numericalize()\n",
        "num.setup(toks200)\n",
        "coll_repr(num.vocab,20)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This vocab has all special words at the beginning followed by each word with a frequency of one.\n",
        "\n",
        "Now we convert the tokenized text into numbers anf grab the first 20"
      ],
      "metadata": {
        "id": "znv7KiNDF6hO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "910ylASZpEgF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04bb7f75-0905-4aa0-dcaa-e91adaadc9fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorText([   2,   19,  173,    9,   32,  593,   12,   24,    0, 1498,   11,    8,    9,   32,  832,    9,    0,   17,    9,   88])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "nums = num(toks)[:20]; nums"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we map these numbers to respective token using the vocab which has its index"
      ],
      "metadata": {
        "id": "fhcQpx1pN9ql"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBd04_J2pEgF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "56a52083-2bf0-4cdc-ac0e-45b70c9b5b36"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"xxbos i saw the film today and was xxunk impressed . xxmaj the film captured the xxunk in the '\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "' '.join(num.vocab[o] for o in nums)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAAmUGdepEgF"
      },
      "source": [
        "### Putting Our Texts into Batches for a Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below takes a stream, tokenizes it and then converts into an array. Tokens are from start index to end index such that these mini-streams are contiguous and we get 6 such arrays.\n",
        "\n",
        "We then convert it into a DataFrame and display as HTML."
      ],
      "metadata": {
        "id": "NPA2HdMUUTlM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZfNsep-pEgG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "f8f2bb3f-b66b-4a43-b107-9163fd00dc84"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>xxbos</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>in</td>\n",
              "      <td>this</td>\n",
              "      <td>chapter</td>\n",
              "      <td>,</td>\n",
              "      <td>we</td>\n",
              "      <td>will</td>\n",
              "      <td>go</td>\n",
              "      <td>back</td>\n",
              "      <td>over</td>\n",
              "      <td>the</td>\n",
              "      <td>example</td>\n",
              "      <td>of</td>\n",
              "      <td>classifying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>movie</td>\n",
              "      <td>reviews</td>\n",
              "      <td>we</td>\n",
              "      <td>studied</td>\n",
              "      <td>in</td>\n",
              "      <td>chapter</td>\n",
              "      <td>1</td>\n",
              "      <td>and</td>\n",
              "      <td>dig</td>\n",
              "      <td>deeper</td>\n",
              "      <td>under</td>\n",
              "      <td>the</td>\n",
              "      <td>surface</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>first</td>\n",
              "      <td>we</td>\n",
              "      <td>will</td>\n",
              "      <td>look</td>\n",
              "      <td>at</td>\n",
              "      <td>the</td>\n",
              "      <td>processing</td>\n",
              "      <td>steps</td>\n",
              "      <td>necessary</td>\n",
              "      <td>to</td>\n",
              "      <td>convert</td>\n",
              "      <td>text</td>\n",
              "      <td>into</td>\n",
              "      <td>numbers</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>how</td>\n",
              "      <td>to</td>\n",
              "      <td>customize</td>\n",
              "      <td>it</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>by</td>\n",
              "      <td>doing</td>\n",
              "      <td>this</td>\n",
              "      <td>,</td>\n",
              "      <td>we</td>\n",
              "      <td>'ll</td>\n",
              "      <td>have</td>\n",
              "      <td>another</td>\n",
              "      <td>example</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>of</td>\n",
              "      <td>the</td>\n",
              "      <td>preprocessor</td>\n",
              "      <td>used</td>\n",
              "      <td>in</td>\n",
              "      <td>the</td>\n",
              "      <td>data</td>\n",
              "      <td>block</td>\n",
              "      <td>xxup</td>\n",
              "      <td>api</td>\n",
              "      <td>.</td>\n",
              "      <td>\\n</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>then</td>\n",
              "      <td>we</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>will</td>\n",
              "      <td>study</td>\n",
              "      <td>how</td>\n",
              "      <td>we</td>\n",
              "      <td>build</td>\n",
              "      <td>a</td>\n",
              "      <td>language</td>\n",
              "      <td>model</td>\n",
              "      <td>and</td>\n",
              "      <td>train</td>\n",
              "      <td>it</td>\n",
              "      <td>for</td>\n",
              "      <td>a</td>\n",
              "      <td>while</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "stream = \"In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the PreProcessor used in the data block API.\\nThen we will study how we build a language model and train it for a while.\"\n",
        "tokens = tkn(stream)\n",
        "bs,seq_len = 6,15\n",
        "d_tokens = np.array([tokens[i*seq_len:(i+1)*seq_len] for i in range(bs)])\n",
        "df = pd.DataFrame(d_tokens)\n",
        "display(HTML(df.to_html(index=False,header=None)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This ğŸ‘‡ is also the same thing but the batches are from 0-5,15-20 and so on i.e. we get the first 5 tokens from the 15 token sentence."
      ],
      "metadata": {
        "id": "353pz9NqVClB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZPGkjMPpEgH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "ba6da729-4146-41d2-afde-2eb57b5ad545"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>xxbos</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>in</td>\n",
              "      <td>this</td>\n",
              "      <td>chapter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>movie</td>\n",
              "      <td>reviews</td>\n",
              "      <td>we</td>\n",
              "      <td>studied</td>\n",
              "      <td>in</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>first</td>\n",
              "      <td>we</td>\n",
              "      <td>will</td>\n",
              "      <td>look</td>\n",
              "      <td>at</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>how</td>\n",
              "      <td>to</td>\n",
              "      <td>customize</td>\n",
              "      <td>it</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>of</td>\n",
              "      <td>the</td>\n",
              "      <td>preprocessor</td>\n",
              "      <td>used</td>\n",
              "      <td>in</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>will</td>\n",
              "      <td>study</td>\n",
              "      <td>how</td>\n",
              "      <td>we</td>\n",
              "      <td>build</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "bs,seq_len = 6,5\n",
        "d_tokens = np.array([tokens[i*15:i*15+seq_len] for i in range(bs)])\n",
        "df = pd.DataFrame(d_tokens)\n",
        "display(HTML(df.to_html(index=False,header=None)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is also same as before but here we get the batches with tokens from 5-10,20-25 and so on i.e. we get the middle part of 15 tokens sentence."
      ],
      "metadata": {
        "id": "Vg3JXU9oVzX7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytvRxOtDpEgH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "5cc073d5-0cc8-4a76-f4c6-a296b0e29d35"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>,</td>\n",
              "      <td>we</td>\n",
              "      <td>will</td>\n",
              "      <td>go</td>\n",
              "      <td>back</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>chapter</td>\n",
              "      <td>1</td>\n",
              "      <td>and</td>\n",
              "      <td>dig</td>\n",
              "      <td>deeper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>the</td>\n",
              "      <td>processing</td>\n",
              "      <td>steps</td>\n",
              "      <td>necessary</td>\n",
              "      <td>to</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>xxmaj</td>\n",
              "      <td>by</td>\n",
              "      <td>doing</td>\n",
              "      <td>this</td>\n",
              "      <td>,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>the</td>\n",
              "      <td>data</td>\n",
              "      <td>block</td>\n",
              "      <td>xxup</td>\n",
              "      <td>api</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>a</td>\n",
              "      <td>language</td>\n",
              "      <td>model</td>\n",
              "      <td>and</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "bs,seq_len = 6,5\n",
        "d_tokens = np.array([tokens[i*15+seq_len:i*15+2*seq_len] for i in range(bs)])\n",
        "df = pd.DataFrame(d_tokens)\n",
        "display(HTML(df.to_html(index=False,header=None)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is also the same thing but here we get batches with tokens from 10-15,25-30 and so on i.e. we get the last part of the 15 token sentence."
      ],
      "metadata": {
        "id": "7er0v1NlWKCw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSpJDgpXpEgI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "256e6639-5160-479f-83a7-56896ed332b6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>over</td>\n",
              "      <td>the</td>\n",
              "      <td>example</td>\n",
              "      <td>of</td>\n",
              "      <td>classifying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>under</td>\n",
              "      <td>the</td>\n",
              "      <td>surface</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>convert</td>\n",
              "      <td>text</td>\n",
              "      <td>into</td>\n",
              "      <td>numbers</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>we</td>\n",
              "      <td>'ll</td>\n",
              "      <td>have</td>\n",
              "      <td>another</td>\n",
              "      <td>example</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>.</td>\n",
              "      <td>\\n</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>then</td>\n",
              "      <td>we</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>it</td>\n",
              "      <td>for</td>\n",
              "      <td>a</td>\n",
              "      <td>while</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "bs,seq_len = 6,5\n",
        "d_tokens = np.array([tokens[i*15+10:i*15+15] for i in range(bs)])\n",
        "df = pd.DataFrame(d_tokens)\n",
        "display(HTML(df.to_html(index=False,header=None)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we numericalize the toks200 i.e. the tokenized 200 files."
      ],
      "metadata": {
        "id": "lecm4qHhWkPO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtgalR_fpEgI"
      },
      "outputs": [],
      "source": [
        "nums200 = toks200.map(num)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nums200[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKOmb9xOW1t0",
        "outputId": "173b4184-49cf-48be-ef8c-a9bb88f992ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorText([   2,   19,  173,    9,   32,  593,   12,   24,    0, 1498,   11,    8,    9,   32,  832,    9,    0,   17,    9,   88, 1499,   62,  979,    0, 1181,    9,  833,  138,  185,   11,    8,\n",
              "              18,   24,   79,   14,   87,    8,    0,    8,    0,   12,    8,    0,    8, 1182,    0,   14, 1500, 1183,  174,   28,    8,    0,   14,  133,   95,    9,    0,   15,    9,  410,   16,\n",
              "               0,   11,    8, 1501,    8,    0,   10,    8,    0,    8,    0,   12,    8,    0,    8,  980,    8,    0,   45,  206,    0,    0,   30,    8,    0,   23,    0,   50,    8,    0,    8,\n",
              "               0,  123,  201,    0,   11,    8,   18,   24,  103,   99,   14,   87,   88, 1499,    0,    8,  834,   12,    8,    0,   23,  202,    8,    0,  123,    0,   11,    8,    9,  981,    0,\n",
              "               9,    0,   10,    0,   26,    0, 1183,  174,   28,    8,  135, 1502,    8,  180,   30,    9,    0,   26, 1184, 1183,  174,   28,    8,    0,   12,    8,    0,   24, 1503,   12,    0,\n",
              "              11,    8,  513,    9,    0,    8,    0,    8,    0, 1504,   17,  471,    0,    0,   29,   13,    0,    0,  318,   24,  268,    0,   11,   19,  201,   53,   95,    9,   32,    0,   95,\n",
              "               9,  207,  336,  109,  381,    0,   10,    0,   12,  193,    0,   11,    8,  186,   53,  738,   10,  514,  337,   17,    0,  208,   41,    0,   17,    9,    0,  153,   75,   51,   96,\n",
              "              14,  472,   18,   12,    0,  657,   58,   45,   15,    9,    0,   24,   63,   11])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a recap,\n",
        "\n",
        "at each epoch,\n",
        "- we shuffle the documents\n",
        "- concatenated them into stream of tokens\n",
        "- cut them down into batches which consists of fixed consecutive mini-streams.\n",
        "- then our model will read the mini-streams in order\n",
        "\n",
        "All these are handled behind the scenes by the `LMDataLoader`. We do this by first applying the numericalize to tokenized text and then pass it to LMDataLoader."
      ],
      "metadata": {
        "id": "Fr9GktEfXbcT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ugXzJwjpEgJ"
      },
      "outputs": [],
      "source": [
        "dl = LMDataLoader(nums200)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then grab the first batch and check shapes"
      ],
      "metadata": {
        "id": "RUkC2-urYmB7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0FV9fMWpEgJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "831b8795-c8ed-44b0-a999-2d9f2030ed44"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 72]), torch.Size([64, 72]))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "x,y = first(dl)\n",
        "x.shape,y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting the independent variables of first row and grabbing few tokens"
      ],
      "metadata": {
        "id": "mfdIddLIYsF3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArDMoBO7pEgJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "f549358f-ca9d-4339-8dfc-2989f8174627"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"xxbos i saw the film today and was xxunk impressed . xxmaj the film captured the xxunk in the '\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "' '.join(num.vocab[o] for o in x[0][:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The same for dependent is same but with an offset of one token"
      ],
      "metadata": {
        "id": "_GeiwQaFY8p2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97iIXvofpEgK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "5fa9f161-04d9-4899-f866-d9e24afad49f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"i saw the film today and was xxunk impressed . xxmaj the film captured the xxunk in the ' 80s\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "' '.join(num.vocab[o] for o in y[0][:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMCTqzbNpEgK"
      },
      "source": [
        "## Training a Text Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-pNyVf-pEgK"
      },
      "source": [
        "### Language Model Using DataBlock"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to fine-tune a pretrained language model, we need to create a DataBlock and pass it the parameters like we did for image classification.\n",
        "\n",
        "fastai does tokenization and numericalization automatically for us when we pass TextBlock as blocks.\n",
        "\n",
        "First we use `partial` and create a new function which is same as `get_text_files` but the folders are fixed.\n",
        "\n",
        "We then create a DataBlock and pass it TextBlock as blocks and set is_lm=True which takes the next word as labe and then create DataLoaders and pass the path of data and path where it should store the tokenized inputs."
      ],
      "metadata": {
        "id": "PVz9x-qy0V1I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utxIH4YYpEgL"
      },
      "outputs": [],
      "source": [
        "get_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])\n",
        "\n",
        "dls_lm = DataBlock(\n",
        "    blocks=TextBlock.from_folder(path, is_lm=True),\n",
        "    get_items=get_imdb, splitter=RandomSplitter(0.1)\n",
        ").dataloaders(path, path=path, bs=128, seq_len=80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtsFHvzLpEga"
      },
      "outputs": [],
      "source": [
        "dls_lm.show_batch(max_n=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9jXHdy7pEgb"
      },
      "source": [
        "### Fine-Tuning the Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then create a language model learner which takes the DataLoaders we created and use the RNNs architecture AWD_LSTM. The embeddings of pretrained model are merged with random embeddings of words that weren't in the pretrained vocab. This is automatically handled by language_model_leaner. These embeddings are fed to the RNNs."
      ],
      "metadata": {
        "id": "HKSm5czc2Wi5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdIHD9vSpEgc"
      },
      "outputs": [],
      "source": [
        "learn = language_model_learner(\n",
        "    dls_lm, AWD_LSTM, drop_mult=0.3,\n",
        "    metrics=[accuracy, Perplexity()]).to_fp16()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To fine-tune, we use fit_one_cycle because it allows us to save intermediate results and we can manually fine-tune the model by gradually unfreezing few layers at a time."
      ],
      "metadata": {
        "id": "jrq8pXSZ3Sp0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MAeYWWbpEgc"
      },
      "outputs": [],
      "source": [
        "learn.fit_one_cycle(1, 2e-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wgo47gBKpEgd"
      },
      "source": [
        "### Saving and Loading Models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save model"
      ],
      "metadata": {
        "id": "_C_tJL9_3qRC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHjojFy_pEgd"
      },
      "outputs": [],
      "source": [
        "learn.save('1epoch')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model"
      ],
      "metadata": {
        "id": "6LyZBEjc3r3y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNnAmIASpEgd"
      },
      "outputs": [],
      "source": [
        "learn = learn.load('1epoch')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfreeze layers of the model and fine-tune"
      ],
      "metadata": {
        "id": "etRS9qBj3tu2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYzjzFoLpEge"
      },
      "outputs": [],
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(10, 2e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the body which is called as encoder in NLP, leaving the task specific head."
      ],
      "metadata": {
        "id": "_0DWme8D3zGR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hv4BPxsepEge"
      },
      "outputs": [],
      "source": [
        "learn.save_encoder('finetuned')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So now we've completed fine-tuning the language model to the IMDb corpus."
      ],
      "metadata": {
        "id": "vunhQXjv3-wO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mpkG5XdpEgf"
      },
      "source": [
        "### Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll see how good the model is at predicting the next word of movie reviews."
      ],
      "metadata": {
        "id": "yWRlTl784GLw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NVVLBkOpEgf"
      },
      "outputs": [],
      "source": [
        "TEXT = \"I liked this movie because\"\n",
        "N_WORDS = 40\n",
        "N_SENTENCES = 2\n",
        "preds = [learn.predict(TEXT, N_WORDS, temperature=0.75)\n",
        "         for _ in range(N_SENTENCES)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjjmklhhpEgf"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\".join(preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niGGnesbpEgg"
      },
      "source": [
        "### Creating the Classifier DataLoaders"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will create a DataLoaders for classifier to fine-tune the language model for classification task.\n",
        "\n",
        "We pass it the vocab of the fine-tuned language model on IMBd corpus so that it uses the same correspondence of token to index. And also that the embedding of pretrained model won't make sense to this classifier model and fine-tuning will not be useful.\n",
        "\n",
        "\n",
        "Sorting and padding are automatically doen by the DataBlock API when we use TextBlock."
      ],
      "metadata": {
        "id": "8qId0XVB4OMV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oP-0AwbpEgg"
      },
      "outputs": [],
      "source": [
        "dls_clas = DataBlock(\n",
        "    blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock),\n",
        "    get_y = parent_label,\n",
        "    get_items=partial(get_text_files, folders=['train', 'test']),\n",
        "    splitter=GrandparentSplitter(valid_name='test')\n",
        ").dataloaders(path, path=path, bs=128, seq_len=72)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWDczjSGpEgg"
      },
      "outputs": [],
      "source": [
        "dls_clas.show_batch(max_n=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56WjlBzUpEgh"
      },
      "outputs": [],
      "source": [
        "nums_samp = toks200[:10].map(num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiBdGi_9pEgh"
      },
      "outputs": [],
      "source": [
        "nums_samp.map(len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XTAJbXvpEgi"
      },
      "outputs": [],
      "source": [
        "learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5,\n",
        "                                metrics=accuracy).to_fp16()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a text Classifier"
      ],
      "metadata": {
        "id": "HJ0QAR_u6ieQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHRF1pavpEgi"
      },
      "outputs": [],
      "source": [
        "learn = learn.load_encoder('finetuned')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we load the encoder for it to fine-tuned."
      ],
      "metadata": {
        "id": "P2zV61Uz6nvH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz1TcNqfpEgi"
      },
      "source": [
        "### Fine-Tuning the Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used for fine-tuning the classfier"
      ],
      "metadata": {
        "id": "7v4VwERY62Y1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7JrJSLlpEgj"
      },
      "outputs": [],
      "source": [
        "learn.fit_one_cycle(1, 2e-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tuning after freezing all layers until specified layers"
      ],
      "metadata": {
        "id": "qIFms09q7AWA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIWX_SxBpEgj"
      },
      "outputs": [],
      "source": [
        "learn.freeze_to(-2)\n",
        "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xumpygyapEgk"
      },
      "outputs": [],
      "source": [
        "learn.freeze_to(-3)\n",
        "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47SgiXzZpEgk"
      },
      "outputs": [],
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6s0RkgwpEgl"
      },
      "source": [
        "## Disinformation and Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpUIq4VqpEgl"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yMoBT80pEgm"
      },
      "source": [
        "## Questionnaire"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCShUdq7pEgm"
      },
      "source": [
        "1. What is \"self-supervised learning\"?\n",
        "1. What is a \"language model\"?\n",
        "1. Why is a language model considered self-supervised?\n",
        "1. What are self-supervised models usually used for?\n",
        "1. Why do we fine-tune language models?\n",
        "1. What are the three steps to create a state-of-the-art text classifier?\n",
        "1. How do the 50,000 unlabeled movie reviews help us create a better text classifier for the IMDb dataset?\n",
        "1. What are the three steps to prepare your data for a language model?\n",
        "1. What is \"tokenization\"? Why do we need it?\n",
        "1. Name three different approaches to tokenization.\n",
        "1. What is `xxbos`?\n",
        "1. List four rules that fastai applies to text during tokenization.\n",
        "1. Why are repeated characters replaced with a token showing the number of repetitions and the character that's repeated?\n",
        "1. What is \"numericalization\"?\n",
        "1. Why might there be words that are replaced with the \"unknown word\" token?\n",
        "1. With a batch size of 64, the first row of the tensor representing the first batch contains the first 64 tokens for the dataset. What does the second row of that tensor contain? What does the first row of the second batch contain? (Carefulâ€”students often get this one wrong! Be sure to check your answer on the book's website.)\n",
        "1. Why do we need padding for text classification? Why don't we need it for language modeling?\n",
        "1. What does an embedding matrix for NLP contain? What is its shape?\n",
        "1. What is \"perplexity\"?\n",
        "1. Why do we have to pass the vocabulary of the language model to the classifier data block?\n",
        "1. What is \"gradual unfreezing\"?\n",
        "1. Why is text generation always likely to be ahead of automatic identification of machine-generated texts?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vF5Usf7RpEgn"
      },
      "source": [
        "### Further Research"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7y0UGqvipEgo"
      },
      "source": [
        "1. See what you can learn about language models and disinformation. What are the best language models today? Take a look at some of their outputs. Do you find them convincing? How could a bad actor best use such a model to create conflict and uncertainty?\n",
        "1. Given the limitation that models are unlikely to be able to consistently recognize machine-generated texts, what other approaches may be needed to handle large-scale disinformation campaigns that leverage deep learning?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcVMNyoppEgo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "jupytext": {
      "split_at_heading": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}