{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPmk1OT/04hDXU1SX+MJYU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shuraimi/fastai_DL_course/blob/main/IMDb_Sentiment_Analysis(Lesson4).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In this notebook, I'll be reproducing the chapter 10 i.e. IMDb Sentiment Analysis.\n",
        "\n",
        "## Steps I'll take\n",
        "0. Imports\n",
        "1. Take a look at tokenization\n",
        "2. Then at numericalization\n",
        "3. Make data ready or assemble data to train language model i.e. create dataloader\n",
        "4. Fine-tune the language model on IMDb corpus\n",
        "5. Fine tune it to get a classifier for sentiment analysis"
      ],
      "metadata": {
        "id": "dlHZSutZUdZl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "vYUWSqHNm4_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#hide\n",
        "! [ -e /content ] && pip install -Uqq fastbook\n",
        "import fastbook\n",
        "fastbook.setup_book()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqQxtfB5m6dg",
        "outputId": "2e325f3e-74eb-45fe-df63-645789816de7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.8/719.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m787.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m206.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2024.5.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mMounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hide\n",
        "from fastbook import *\n",
        "from IPython.display import display,HTML"
      ],
      "metadata": {
        "id": "glja2GmknC7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastai.text.all import *"
      ],
      "metadata": {
        "id": "kDwn1hN3nFdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "3X2tfrcGmGZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First download and extract the data"
      ],
      "metadata": {
        "id": "nYla7aHMmk6q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uej9h2HoUQsv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "da03cdb7-0790-4548-99da-b160c88dc32d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='144441344' class='' max='144440600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [144441344/144440600 00:02&lt;00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "path=untar_data(URLs.IMDB)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# looking at the folders in the path\n",
        "path.ls()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjr6FMuFng5Y",
        "outputId": "7ad157bb-f8eb-485a-dbcf-3ee7b46fa717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#7) [Path('/root/.fastai/data/imdb/test'),Path('/root/.fastai/data/imdb/train'),Path('/root/.fastai/data/imdb/imdb.vocab'),Path('/root/.fastai/data/imdb/unsup'),Path('/root/.fastai/data/imdb/README'),Path('/root/.fastai/data/imdb/tmp_lm'),Path('/root/.fastai/data/imdb/tmp_clas')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get all text files\n",
        "files=get_text_files(path,folders=['train','test','unsup'])"
      ],
      "metadata": {
        "id": "2YNgfI9znmBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# open a file and read it and display a part of it\n",
        "txt=files[0].open().read()\n",
        "txt[:50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Cron-V08n6zV",
        "outputId": "67569efb-3768-40d2-d3db-42d9fc55b0e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Do-It-Yourself indie horror auteur Todd Sheets ret'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word tokenization"
      ],
      "metadata": {
        "id": "6GiHHhdfwv8p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For tokenizing, we use `SpacyTokenizer` which was the tokenizer used at first or we can use the `WordTokenizer` by fastai which refers to the default tokenizer at present.\n",
        "\n",
        "We need to pass the txt as a list."
      ],
      "metadata": {
        "id": "7mL25V7Soteq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spacy=SpacyTokenizer()\n",
        "tkn=first(spacy([txt]))"
      ],
      "metadata": {
        "id": "P9_9kf4uo-_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tkn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ibZuZalpQE3",
        "outputId": "b0d45bd6-9365-41b5-e0fd-ada9d744c82a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#364) ['Do','-','It','-','Yourself','indie','horror','auteur','Todd','Sheets'...]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or we can use the `WordTokenizer`"
      ],
      "metadata": {
        "id": "xq1fy3mQpYU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wt=WordTokenizer()\n",
        "tkns=first(wt([txt]))\n",
        "tkns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYqm3l71pc_y",
        "outputId": "254efa2d-c97b-459d-9932-ba4b2b34dd2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#364) ['Do','-','It','-','Yourself','indie','horror','auteur','Todd','Sheets'...]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other than these, fastai provides a `Tokenizer` class which provides few additional functionality to tokenization process such as adding special tokens while tokenization.\n",
        "\n",
        "We pass the word tokenizer to this class to specify word based tokenization strategy while leveraging the additional functionality of fastai's `Tokenizer`."
      ],
      "metadata": {
        "id": "O9lcbdHLpuUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tknr=Tokenizer(wt)\n",
        "print(coll_repr(tknr(txt),35))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4s1mbYu3p8-B",
        "outputId": "ad500c82-db63-47d3-fa0c-aa537ee223ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(#379) ['xxbos','do','-','it','-','yourself','indie','horror','auteur','xxmaj','todd','xxmaj','sheets','returns','with','another','entertainingly','atrocious',\"nickel'n'dime\",'shot','-','on','-','video','clunker','that',\"'s\",'basically','just','a','feeble','excuse','to','sling','around'...]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(Tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bu5PdGZdrBsX",
        "outputId": "0cd21ae9-73c3-460c-9165-71d797c3d8d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class Tokenizer in module fastai.text.core:\n",
            "\n",
            "class Tokenizer(fastcore.transform.Transform)\n",
            " |  Tokenizer(self, tok, rules=None, counter=None, lengths=None, mode=None, sep=' ')\n",
            " |  \n",
            " |  Provides a consistent `Transform` interface to tokenizers operating on `DataFrame`s and folders\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      Tokenizer\n",
            " |      fastcore.transform.Transform\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, tok, rules=None, counter=None, lengths=None, mode=None, sep=' ')\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  decodes = (object,object) -> decodes\n",
            " |  \n",
            " |  encodes = (Path,object) -> encodes\n",
            " |  (str,object) -> encodes\n",
            " |  \n",
            " |  get_lengths(self, items)\n",
            " |  \n",
            " |  setups = (object,object) -> setups\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  from_df(text_cols, tok=None, rules=None, sep=' ', *, n_workers=2, mark_fields=None, tok_text_col='text', **kwargs) from fastcore.transform._TfmMeta\n",
            " |  \n",
            " |  from_folder(path, tok=None, rules=None, *, extensions=None, folders=None, output_dir=None, skip_if_exists=True, output_names=None, n_workers=2, encoding='utf8', **kwargs) from fastcore.transform._TfmMeta\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __signature__ = <Signature (self, tok, rules=None, counter=None, lengt...\n",
            " |  \n",
            " |  input_types = (<class 'str'>, <class 'list'>, <class 'fastcore.foundat...\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from fastcore.transform.Transform:\n",
            " |  \n",
            " |  __call__(self, x, **kwargs)\n",
            " |      Call self as a function.\n",
            " |  \n",
            " |  __repr__(self)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  decode(self, x, **kwargs)\n",
            " |      Delegate to <code>decodes</code> to undo transform\n",
            " |  \n",
            " |  setup(self, items=None, train_setup=False)\n",
            " |      Delegate to <code>setups</code> to set up transform\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from fastcore.transform.Transform:\n",
            " |  \n",
            " |  name\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from fastcore.transform.Transform:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from fastcore.transform.Transform:\n",
            " |  \n",
            " |  init_enc = None\n",
            " |  \n",
            " |  order = 0\n",
            " |  \n",
            " |  split_idx = None\n",
            " |  \n",
            " |  train_setup = None\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Subword tokenization\n",
        "\n",
        "We define a function for subword tokenization which\n",
        "- instantiates the SubwordTokenizer and takes vocab size\n",
        "- runs setup on txts\n",
        "- and returns tokens of subwords joined\n",
        "\n",
        "We grab the 200 files inorder to train the subword tokenizer to find the most commonly occuring group of letters in words."
      ],
      "metadata": {
        "id": "5GAIMWF0w0HC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txts=L(o.open().read() for o in files[:2000])"
      ],
      "metadata": {
        "id": "urHtXEY6xd7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txts[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "9hRSNReh3pkM",
        "outputId": "f96c2dc8-3c87-4cd0-8c65-fe45f04127d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Do-It-Yourself indie horror auteur Todd Sheets returns with another entertainingly atrocious nickel'n'dime shot-on-video clunker that's basically just a feeble excuse to sling around a lot of watery blood and gleaming guts as often as possible. An evil demonic scarecrow resurrects the dead as ravenous rot-faced zombies so they can feast on the living. A bunch of bickering college kids, a trio of dangerous escaped convicts led by the vicious Slade (Byron Nichodemus hamming it up to an outrageous degree), two equally savage sleazeball hoodlums, and a trio of hottie sisters all have to do their best to survive this harrowing ordeal. That's it for the needlessly muddled and convoluted plot, but fortunately what this hilariously horrendous hoot lacks in narrative coherence (plenty) it more than compensates for with a pleasing plethora of gloriously gross'n'graphic gore. Disgusting highlights include a woman having her fingers chopped off, a fatal gunshot to a young gal's groin, attempted necrophiliac rape, evisceration, and, of course, more repulsive entrail eating than you can shake a pile of moist intestines at. Moreover, we've also got rough, grainy cinematography that constantly alternates between washed-out color and grimy black and white, ineptly staged fight scenes, lousy acting from a uniformly pathetic no-name cast (Jerry Angell in particular cops the top crummy thespic dishonors for his laughably abysmal histrionics as slimy no-count psycho criminal Joe Bob), a grating head-banging thrash metal soundtrack, and a generic shivery'n'ominous synthesizer score. Let's not forget the ridiculous ending in which several of our survivors stumble across a few vials of flesh-eating bacteria to use on the shambling undead hordes. Sure, this flick is pure dreck, but it has a certain endearingly abominable quality to it that in turn makes it a great deal of so-awful-it's-awesome Grade Z fun for hardcore aficionados of bad fright fare.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function for subword tokenization\n",
        "def subword_tokenization(sz):\n",
        "    sub=SubwordTokenizer(vocab_sz=sz)\n",
        "    sub.setup(txts)\n",
        "    return ' '.join(first(sub([txt])))[:40]"
      ],
      "metadata": {
        "id": "abNe90TVqXX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subword_tokenization(200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "9YVUabmJx8Md",
        "outputId": "fb4c40c8-218d-4a41-ea0a-87fa68af88f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'▁ D o - I t - Y o ur s el f ▁in d i e ▁ '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have seen how we do tokenization, we need to convert them into numbers."
      ],
      "metadata": {
        "id": "QAexspuySi7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Numericalization\n",
        "\n",
        "1. Create vocab\n",
        "2. Convert the tokens into numbers"
      ],
      "metadata": {
        "id": "nQ28kRTLNjYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num=Numericalize()\n",
        "num.setup(txts)"
      ],
      "metadata": {
        "id": "-zdBJ4_1Nl-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numericalized_text=num(txt)"
      ],
      "metadata": {
        "id": "WmBW2cEaRNor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numericalized_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66s7LHAyRW2Q",
        "outputId": "9db9bc51-b6cf-418b-cee7-119fd94b782c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorText([48, 13, 43,  ..., 17, 10, 30])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we map these numbers in vocab to see whether we get the same text as before."
      ],
      "metadata": {
        "id": "vMJukHLpRcEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num.vocab[48]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "dl7mAsIJRkrp",
        "outputId": "de96909a-9da8-4da7-f6bb-f0d50d82234e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'D'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=[num.vocab[i] for i in numericalized_text]"
      ],
      "metadata": {
        "id": "ragCg9kzRw6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YgKRX-2R-YW",
        "outputId": "8e0a7281-2d98-4475-b316-d0b62f88b8bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['D',\n",
              " 'o',\n",
              " '-',\n",
              " 'I',\n",
              " 't',\n",
              " '-',\n",
              " 'Y',\n",
              " 'o',\n",
              " 'u',\n",
              " 'r',\n",
              " 's',\n",
              " 'e',\n",
              " 'l',\n",
              " 'f',\n",
              " ' ',\n",
              " 'i',\n",
              " 'n',\n",
              " 'd',\n",
              " 'i',\n",
              " 'e',\n",
              " ' ',\n",
              " 'h',\n",
              " 'o',\n",
              " 'r',\n",
              " 'r',\n",
              " 'o',\n",
              " 'r',\n",
              " ' ',\n",
              " 'a',\n",
              " 'u',\n",
              " 't',\n",
              " 'e',\n",
              " 'u',\n",
              " 'r',\n",
              " ' ',\n",
              " 'T',\n",
              " 'o',\n",
              " 'd',\n",
              " 'd',\n",
              " ' ',\n",
              " 'S',\n",
              " 'h',\n",
              " 'e',\n",
              " 'e',\n",
              " 't',\n",
              " 's',\n",
              " ' ',\n",
              " 'r',\n",
              " 'e',\n",
              " 't',\n",
              " 'u',\n",
              " 'r',\n",
              " 'n',\n",
              " 's',\n",
              " ' ',\n",
              " 'w',\n",
              " 'i',\n",
              " 't',\n",
              " 'h',\n",
              " ' ',\n",
              " 'a',\n",
              " 'n',\n",
              " 'o',\n",
              " 't',\n",
              " 'h',\n",
              " 'e',\n",
              " 'r',\n",
              " ' ',\n",
              " 'e',\n",
              " 'n',\n",
              " 't',\n",
              " 'e',\n",
              " 'r',\n",
              " 't',\n",
              " 'a',\n",
              " 'i',\n",
              " 'n',\n",
              " 'i',\n",
              " 'n',\n",
              " 'g',\n",
              " 'l',\n",
              " 'y',\n",
              " ' ',\n",
              " 'a',\n",
              " 't',\n",
              " 'r',\n",
              " 'o',\n",
              " 'c',\n",
              " 'i',\n",
              " 'o',\n",
              " 'u',\n",
              " 's',\n",
              " ' ',\n",
              " 'n',\n",
              " 'i',\n",
              " 'c',\n",
              " 'k',\n",
              " 'e',\n",
              " 'l',\n",
              " \"'\",\n",
              " 'n',\n",
              " \"'\",\n",
              " 'd',\n",
              " 'i',\n",
              " 'm',\n",
              " 'e',\n",
              " ' ',\n",
              " 's',\n",
              " 'h',\n",
              " 'o',\n",
              " 't',\n",
              " '-',\n",
              " 'o',\n",
              " 'n',\n",
              " '-',\n",
              " 'v',\n",
              " 'i',\n",
              " 'd',\n",
              " 'e',\n",
              " 'o',\n",
              " ' ',\n",
              " 'c',\n",
              " 'l',\n",
              " 'u',\n",
              " 'n',\n",
              " 'k',\n",
              " 'e',\n",
              " 'r',\n",
              " ' ',\n",
              " 't',\n",
              " 'h',\n",
              " 'a',\n",
              " 't',\n",
              " \"'\",\n",
              " 's',\n",
              " ' ',\n",
              " 'b',\n",
              " 'a',\n",
              " 's',\n",
              " 'i',\n",
              " 'c',\n",
              " 'a',\n",
              " 'l',\n",
              " 'l',\n",
              " 'y',\n",
              " ' ',\n",
              " 'j',\n",
              " 'u',\n",
              " 's',\n",
              " 't',\n",
              " ' ',\n",
              " 'a',\n",
              " ' ',\n",
              " 'f',\n",
              " 'e',\n",
              " 'e',\n",
              " 'b',\n",
              " 'l',\n",
              " 'e',\n",
              " ' ',\n",
              " 'e',\n",
              " 'x',\n",
              " 'c',\n",
              " 'u',\n",
              " 's',\n",
              " 'e',\n",
              " ' ',\n",
              " 't',\n",
              " 'o',\n",
              " ' ',\n",
              " 's',\n",
              " 'l',\n",
              " 'i',\n",
              " 'n',\n",
              " 'g',\n",
              " ' ',\n",
              " 'a',\n",
              " 'r',\n",
              " 'o',\n",
              " 'u',\n",
              " 'n',\n",
              " 'd',\n",
              " ' ',\n",
              " 'a',\n",
              " ' ',\n",
              " 'l',\n",
              " 'o',\n",
              " 't',\n",
              " ' ',\n",
              " 'o',\n",
              " 'f',\n",
              " ' ',\n",
              " 'w',\n",
              " 'a',\n",
              " 't',\n",
              " 'e',\n",
              " 'r',\n",
              " 'y',\n",
              " ' ',\n",
              " 'b',\n",
              " 'l',\n",
              " 'o',\n",
              " 'o',\n",
              " 'd',\n",
              " ' ',\n",
              " 'a',\n",
              " 'n',\n",
              " 'd',\n",
              " ' ',\n",
              " 'g',\n",
              " 'l',\n",
              " 'e',\n",
              " 'a',\n",
              " 'm',\n",
              " 'i',\n",
              " 'n',\n",
              " 'g',\n",
              " ' ',\n",
              " 'g',\n",
              " 'u',\n",
              " 't',\n",
              " 's',\n",
              " ' ',\n",
              " 'a',\n",
              " 's',\n",
              " ' ',\n",
              " 'o',\n",
              " 'f',\n",
              " 't',\n",
              " 'e',\n",
              " 'n',\n",
              " ' ',\n",
              " 'a',\n",
              " 's',\n",
              " ' ',\n",
              " 'p',\n",
              " 'o',\n",
              " 's',\n",
              " 's',\n",
              " 'i',\n",
              " 'b',\n",
              " 'l',\n",
              " 'e',\n",
              " '.',\n",
              " ' ',\n",
              " 'A',\n",
              " 'n',\n",
              " ' ',\n",
              " 'e',\n",
              " 'v',\n",
              " 'i',\n",
              " 'l',\n",
              " ' ',\n",
              " 'd',\n",
              " 'e',\n",
              " 'm',\n",
              " 'o',\n",
              " 'n',\n",
              " 'i',\n",
              " 'c',\n",
              " ' ',\n",
              " 's',\n",
              " 'c',\n",
              " 'a',\n",
              " 'r',\n",
              " 'e',\n",
              " 'c',\n",
              " 'r',\n",
              " 'o',\n",
              " 'w',\n",
              " ' ',\n",
              " 'r',\n",
              " 'e',\n",
              " 's',\n",
              " 'u',\n",
              " 'r',\n",
              " 'r',\n",
              " 'e',\n",
              " 'c',\n",
              " 't',\n",
              " 's',\n",
              " ' ',\n",
              " 't',\n",
              " 'h',\n",
              " 'e',\n",
              " ' ',\n",
              " 'd',\n",
              " 'e',\n",
              " 'a',\n",
              " 'd',\n",
              " ' ',\n",
              " 'a',\n",
              " 's',\n",
              " ' ',\n",
              " 'r',\n",
              " 'a',\n",
              " 'v',\n",
              " 'e',\n",
              " 'n',\n",
              " 'o',\n",
              " 'u',\n",
              " 's',\n",
              " ' ',\n",
              " 'r',\n",
              " 'o',\n",
              " 't',\n",
              " '-',\n",
              " 'f',\n",
              " 'a',\n",
              " 'c',\n",
              " 'e',\n",
              " 'd',\n",
              " ' ',\n",
              " 'z',\n",
              " 'o',\n",
              " 'm',\n",
              " 'b',\n",
              " 'i',\n",
              " 'e',\n",
              " 's',\n",
              " ' ',\n",
              " 's',\n",
              " 'o',\n",
              " ' ',\n",
              " 't',\n",
              " 'h',\n",
              " 'e',\n",
              " 'y',\n",
              " ' ',\n",
              " 'c',\n",
              " 'a',\n",
              " 'n',\n",
              " ' ',\n",
              " 'f',\n",
              " 'e',\n",
              " 'a',\n",
              " 's',\n",
              " 't',\n",
              " ' ',\n",
              " 'o',\n",
              " 'n',\n",
              " ' ',\n",
              " 't',\n",
              " 'h',\n",
              " 'e',\n",
              " ' ',\n",
              " 'l',\n",
              " 'i',\n",
              " 'v',\n",
              " 'i',\n",
              " 'n',\n",
              " 'g',\n",
              " '.',\n",
              " ' ',\n",
              " 'A',\n",
              " ' ',\n",
              " 'b',\n",
              " 'u',\n",
              " 'n',\n",
              " 'c',\n",
              " 'h',\n",
              " ' ',\n",
              " 'o',\n",
              " 'f',\n",
              " ' ',\n",
              " 'b',\n",
              " 'i',\n",
              " 'c',\n",
              " 'k',\n",
              " 'e',\n",
              " 'r',\n",
              " 'i',\n",
              " 'n',\n",
              " 'g',\n",
              " ' ',\n",
              " 'c',\n",
              " 'o',\n",
              " 'l',\n",
              " 'l',\n",
              " 'e',\n",
              " 'g',\n",
              " 'e',\n",
              " ' ',\n",
              " 'k',\n",
              " 'i',\n",
              " 'd',\n",
              " 's',\n",
              " ',',\n",
              " ' ',\n",
              " 'a',\n",
              " ' ',\n",
              " 't',\n",
              " 'r',\n",
              " 'i',\n",
              " 'o',\n",
              " ' ',\n",
              " 'o',\n",
              " 'f',\n",
              " ' ',\n",
              " 'd',\n",
              " 'a',\n",
              " 'n',\n",
              " 'g',\n",
              " 'e',\n",
              " 'r',\n",
              " 'o',\n",
              " 'u',\n",
              " 's',\n",
              " ' ',\n",
              " 'e',\n",
              " 's',\n",
              " 'c',\n",
              " 'a',\n",
              " 'p',\n",
              " 'e',\n",
              " 'd',\n",
              " ' ',\n",
              " 'c',\n",
              " 'o',\n",
              " 'n',\n",
              " 'v',\n",
              " 'i',\n",
              " 'c',\n",
              " 't',\n",
              " 's',\n",
              " ' ',\n",
              " 'l',\n",
              " 'e',\n",
              " 'd',\n",
              " ' ',\n",
              " 'b',\n",
              " 'y',\n",
              " ' ',\n",
              " 't',\n",
              " 'h',\n",
              " 'e',\n",
              " ' ',\n",
              " 'v',\n",
              " 'i',\n",
              " 'c',\n",
              " 'i',\n",
              " 'o',\n",
              " 'u',\n",
              " 's',\n",
              " ' ',\n",
              " 'S',\n",
              " 'l',\n",
              " 'a',\n",
              " 'd',\n",
              " 'e',\n",
              " ' ',\n",
              " '(',\n",
              " 'B',\n",
              " 'y',\n",
              " 'r',\n",
              " 'o',\n",
              " 'n',\n",
              " ' ',\n",
              " 'N',\n",
              " 'i',\n",
              " 'c',\n",
              " 'h',\n",
              " 'o',\n",
              " 'd',\n",
              " 'e',\n",
              " 'm',\n",
              " 'u',\n",
              " 's',\n",
              " ' ',\n",
              " 'h',\n",
              " 'a',\n",
              " 'm',\n",
              " 'm',\n",
              " 'i',\n",
              " 'n',\n",
              " 'g',\n",
              " ' ',\n",
              " 'i',\n",
              " 't',\n",
              " ' ',\n",
              " 'u',\n",
              " 'p',\n",
              " ' ',\n",
              " 't',\n",
              " 'o',\n",
              " ' ',\n",
              " 'a',\n",
              " 'n',\n",
              " ' ',\n",
              " 'o',\n",
              " 'u',\n",
              " 't',\n",
              " 'r',\n",
              " 'a',\n",
              " 'g',\n",
              " 'e',\n",
              " 'o',\n",
              " 'u',\n",
              " 's',\n",
              " ' ',\n",
              " 'd',\n",
              " 'e',\n",
              " 'g',\n",
              " 'r',\n",
              " 'e',\n",
              " 'e',\n",
              " ')',\n",
              " ',',\n",
              " ' ',\n",
              " 't',\n",
              " 'w',\n",
              " 'o',\n",
              " ' ',\n",
              " 'e',\n",
              " 'q',\n",
              " 'u',\n",
              " 'a',\n",
              " 'l',\n",
              " 'l',\n",
              " 'y',\n",
              " ' ',\n",
              " 's',\n",
              " 'a',\n",
              " 'v',\n",
              " 'a',\n",
              " 'g',\n",
              " 'e',\n",
              " ' ',\n",
              " 's',\n",
              " 'l',\n",
              " 'e',\n",
              " 'a',\n",
              " 'z',\n",
              " 'e',\n",
              " 'b',\n",
              " 'a',\n",
              " 'l',\n",
              " 'l',\n",
              " ' ',\n",
              " 'h',\n",
              " 'o',\n",
              " 'o',\n",
              " 'd',\n",
              " 'l',\n",
              " 'u',\n",
              " 'm',\n",
              " 's',\n",
              " ',',\n",
              " ' ',\n",
              " 'a',\n",
              " 'n',\n",
              " 'd',\n",
              " ' ',\n",
              " 'a',\n",
              " ' ',\n",
              " 't',\n",
              " 'r',\n",
              " 'i',\n",
              " 'o',\n",
              " ' ',\n",
              " 'o',\n",
              " 'f',\n",
              " ' ',\n",
              " 'h',\n",
              " 'o',\n",
              " 't',\n",
              " 't',\n",
              " 'i',\n",
              " 'e',\n",
              " ' ',\n",
              " 's',\n",
              " 'i',\n",
              " 's',\n",
              " 't',\n",
              " 'e',\n",
              " 'r',\n",
              " 's',\n",
              " ' ',\n",
              " 'a',\n",
              " 'l',\n",
              " 'l',\n",
              " ' ',\n",
              " 'h',\n",
              " 'a',\n",
              " 'v',\n",
              " 'e',\n",
              " ' ',\n",
              " 't',\n",
              " 'o',\n",
              " ' ',\n",
              " 'd',\n",
              " 'o',\n",
              " ' ',\n",
              " 't',\n",
              " 'h',\n",
              " 'e',\n",
              " 'i',\n",
              " 'r',\n",
              " ' ',\n",
              " 'b',\n",
              " 'e',\n",
              " 's',\n",
              " 't',\n",
              " ' ',\n",
              " 't',\n",
              " 'o',\n",
              " ' ',\n",
              " 's',\n",
              " 'u',\n",
              " 'r',\n",
              " 'v',\n",
              " 'i',\n",
              " 'v',\n",
              " 'e',\n",
              " ' ',\n",
              " 't',\n",
              " 'h',\n",
              " 'i',\n",
              " 's',\n",
              " ' ',\n",
              " 'h',\n",
              " 'a',\n",
              " 'r',\n",
              " 'r',\n",
              " 'o',\n",
              " 'w',\n",
              " 'i',\n",
              " 'n',\n",
              " 'g',\n",
              " ' ',\n",
              " 'o',\n",
              " 'r',\n",
              " 'd',\n",
              " 'e',\n",
              " 'a',\n",
              " 'l',\n",
              " '.',\n",
              " ' ',\n",
              " 'T',\n",
              " 'h',\n",
              " 'a',\n",
              " 't',\n",
              " \"'\",\n",
              " 's',\n",
              " ' ',\n",
              " 'i',\n",
              " 't',\n",
              " ' ',\n",
              " 'f',\n",
              " 'o',\n",
              " 'r',\n",
              " ' ',\n",
              " 't',\n",
              " 'h',\n",
              " 'e',\n",
              " ' ',\n",
              " 'n',\n",
              " 'e',\n",
              " 'e',\n",
              " 'd',\n",
              " 'l',\n",
              " 'e',\n",
              " 's',\n",
              " 's',\n",
              " 'l',\n",
              " 'y',\n",
              " ' ',\n",
              " 'm',\n",
              " 'u',\n",
              " 'd',\n",
              " 'd',\n",
              " 'l',\n",
              " 'e',\n",
              " 'd',\n",
              " ' ',\n",
              " 'a',\n",
              " 'n',\n",
              " 'd',\n",
              " ' ',\n",
              " 'c',\n",
              " 'o',\n",
              " 'n',\n",
              " 'v',\n",
              " 'o',\n",
              " 'l',\n",
              " 'u',\n",
              " 't',\n",
              " 'e',\n",
              " 'd',\n",
              " ' ',\n",
              " 'p',\n",
              " 'l',\n",
              " 'o',\n",
              " 't',\n",
              " ',',\n",
              " ' ',\n",
              " 'b',\n",
              " 'u',\n",
              " 't',\n",
              " ' ',\n",
              " 'f',\n",
              " 'o',\n",
              " 'r',\n",
              " 't',\n",
              " 'u',\n",
              " 'n',\n",
              " 'a',\n",
              " 't',\n",
              " 'e',\n",
              " 'l',\n",
              " 'y',\n",
              " ' ',\n",
              " 'w',\n",
              " 'h',\n",
              " 'a',\n",
              " 't',\n",
              " ' ',\n",
              " 't',\n",
              " 'h',\n",
              " 'i',\n",
              " 's',\n",
              " ' ',\n",
              " 'h',\n",
              " 'i',\n",
              " 'l',\n",
              " 'a',\n",
              " 'r',\n",
              " 'i',\n",
              " 'o',\n",
              " 'u',\n",
              " 's',\n",
              " 'l',\n",
              " 'y',\n",
              " ' ',\n",
              " 'h',\n",
              " 'o',\n",
              " 'r',\n",
              " 'r',\n",
              " 'e',\n",
              " 'n',\n",
              " 'd',\n",
              " 'o',\n",
              " 'u',\n",
              " 's',\n",
              " ' ',\n",
              " 'h',\n",
              " 'o',\n",
              " 'o',\n",
              " 't',\n",
              " ' ',\n",
              " 'l',\n",
              " 'a',\n",
              " 'c',\n",
              " 'k',\n",
              " 's',\n",
              " ' ',\n",
              " 'i',\n",
              " 'n',\n",
              " ' ',\n",
              " 'n',\n",
              " 'a',\n",
              " 'r',\n",
              " 'r',\n",
              " 'a',\n",
              " 't',\n",
              " 'i',\n",
              " 'v',\n",
              " 'e',\n",
              " ' ',\n",
              " 'c',\n",
              " 'o',\n",
              " 'h',\n",
              " 'e',\n",
              " 'r',\n",
              " 'e',\n",
              " 'n',\n",
              " 'c',\n",
              " 'e',\n",
              " ' ',\n",
              " '(',\n",
              " 'p',\n",
              " 'l',\n",
              " 'e',\n",
              " 'n',\n",
              " 't',\n",
              " 'y',\n",
              " ')',\n",
              " ' ',\n",
              " 'i',\n",
              " 't',\n",
              " ' ',\n",
              " 'm',\n",
              " 'o',\n",
              " 'r',\n",
              " 'e',\n",
              " ' ',\n",
              " 't',\n",
              " 'h',\n",
              " 'a',\n",
              " 'n',\n",
              " ' ',\n",
              " 'c',\n",
              " 'o',\n",
              " 'm',\n",
              " 'p',\n",
              " 'e',\n",
              " 'n',\n",
              " 's',\n",
              " 'a',\n",
              " 't',\n",
              " 'e',\n",
              " 's',\n",
              " ' ',\n",
              " 'f',\n",
              " 'o',\n",
              " 'r',\n",
              " ' ',\n",
              " 'w',\n",
              " 'i',\n",
              " 't',\n",
              " 'h',\n",
              " ' ',\n",
              " 'a',\n",
              " ' ',\n",
              " 'p',\n",
              " 'l',\n",
              " 'e',\n",
              " 'a',\n",
              " 's',\n",
              " 'i',\n",
              " 'n',\n",
              " 'g',\n",
              " ' ',\n",
              " 'p',\n",
              " 'l',\n",
              " 'e',\n",
              " 't',\n",
              " 'h',\n",
              " 'o',\n",
              " 'r',\n",
              " 'a',\n",
              " ' ',\n",
              " 'o',\n",
              " 'f',\n",
              " ' ',\n",
              " 'g',\n",
              " 'l',\n",
              " 'o',\n",
              " 'r',\n",
              " 'i',\n",
              " 'o',\n",
              " 'u',\n",
              " 's',\n",
              " 'l',\n",
              " 'y',\n",
              " ' ',\n",
              " 'g',\n",
              " 'r',\n",
              " 'o',\n",
              " 's',\n",
              " 's',\n",
              " \"'\",\n",
              " 'n',\n",
              " \"'\",\n",
              " 'g',\n",
              " 'r',\n",
              " 'a',\n",
              " 'p',\n",
              " 'h',\n",
              " 'i',\n",
              " 'c',\n",
              " ' ',\n",
              " 'g',\n",
              " 'o',\n",
              " 'r',\n",
              " 'e',\n",
              " '.',\n",
              " ' ',\n",
              " 'D',\n",
              " 'i',\n",
              " 's',\n",
              " 'g',\n",
              " 'u',\n",
              " 's',\n",
              " 't',\n",
              " 'i',\n",
              " 'n',\n",
              " 'g',\n",
              " ' ',\n",
              " 'h',\n",
              " 'i',\n",
              " 'g',\n",
              " 'h',\n",
              " 'l',\n",
              " 'i',\n",
              " 'g',\n",
              " 'h',\n",
              " 't',\n",
              " 's',\n",
              " ' ',\n",
              " 'i',\n",
              " 'n',\n",
              " 'c',\n",
              " 'l',\n",
              " 'u',\n",
              " 'd',\n",
              " 'e',\n",
              " ' ',\n",
              " 'a',\n",
              " ' ',\n",
              " 'w',\n",
              " 'o',\n",
              " 'm',\n",
              " 'a',\n",
              " 'n',\n",
              " ' ',\n",
              " 'h',\n",
              " 'a',\n",
              " 'v',\n",
              " 'i',\n",
              " 'n',\n",
              " 'g',\n",
              " ' ',\n",
              " 'h',\n",
              " 'e',\n",
              " 'r',\n",
              " ' ',\n",
              " 'f',\n",
              " 'i',\n",
              " 'n',\n",
              " 'g',\n",
              " 'e',\n",
              " 'r',\n",
              " 's',\n",
              " ' ',\n",
              " 'c',\n",
              " 'h',\n",
              " 'o',\n",
              " 'p',\n",
              " 'p',\n",
              " 'e',\n",
              " 'd',\n",
              " ' ',\n",
              " 'o',\n",
              " 'f',\n",
              " 'f',\n",
              " ',',\n",
              " ' ',\n",
              " 'a',\n",
              " ' ',\n",
              " 'f',\n",
              " 'a',\n",
              " 't',\n",
              " 'a',\n",
              " 'l',\n",
              " ' ',\n",
              " 'g',\n",
              " 'u',\n",
              " 'n',\n",
              " 's',\n",
              " 'h',\n",
              " 'o',\n",
              " 't',\n",
              " ' ',\n",
              " 't',\n",
              " 'o',\n",
              " ' ',\n",
              " 'a',\n",
              " ' ',\n",
              " 'y',\n",
              " 'o',\n",
              " 'u',\n",
              " 'n',\n",
              " 'g',\n",
              " ' ',\n",
              " 'g',\n",
              " 'a',\n",
              " 'l',\n",
              " \"'\",\n",
              " 's',\n",
              " ' ',\n",
              " 'g',\n",
              " 'r',\n",
              " 'o',\n",
              " 'i',\n",
              " 'n',\n",
              " ',',\n",
              " ' ',\n",
              " 'a',\n",
              " 't',\n",
              " 't',\n",
              " 'e',\n",
              " 'm',\n",
              " 'p',\n",
              " 't',\n",
              " 'e',\n",
              " 'd',\n",
              " ' ',\n",
              " 'n',\n",
              " 'e',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''.join(num.vocab[i] for i in numericalized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "oRIVunuKSChB",
        "outputId": "af10860f-0d54-46f3-8788-30ed3217ff3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Do-It-Yourself indie horror auteur Todd Sheets returns with another entertainingly atrocious nickel'n'dime shot-on-video clunker that's basically just a feeble excuse to sling around a lot of watery blood and gleaming guts as often as possible. An evil demonic scarecrow resurrects the dead as ravenous rot-faced zombies so they can feast on the living. A bunch of bickering college kids, a trio of dangerous escaped convicts led by the vicious Slade (Byron Nichodemus hamming it up to an outrageous degree), two equally savage sleazeball hoodlums, and a trio of hottie sisters all have to do their best to survive this harrowing ordeal. That's it for the needlessly muddled and convoluted plot, but fortunately what this hilariously horrendous hoot lacks in narrative coherence (plenty) it more than compensates for with a pleasing plethora of gloriously gross'n'graphic gore. Disgusting highlights include a woman having her fingers chopped off, a fatal gunshot to a young gal's groin, attempted necrophiliac rape, evisceration, and, of course, more repulsive entrail eating than you can shake a pile of moist intestines at. Moreover, we've also got rough, grainy cinematography that constantly alternates between washed-out color and grimy black and white, ineptly staged fight scenes, lousy acting from a uniformly pathetic no-name cast (Jerry Angell in particular cops the top crummy thespic dishonors for his laughably abysmal histrionics as slimy no-count psycho criminal Joe Bob), a grating head-banging thrash metal soundtrack, and a generic shivery'n'ominous synthesizer score. Let's not forget the ridiculous ending in which several of our survivors stumble across a few vials of flesh-eating bacteria to use on the shambling undead hordes. Sure, this flick is pure dreck, but it has a certain endearingly abominable quality to it that in turn makes it a great deal of so-awful-it's-awesome Grade Z fun for hardcore aficionados of bad fright fare.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we numbers, we need to make batches."
      ],
      "metadata": {
        "id": "2MJFmYWnSdZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting our text into batches to create Language model\n",
        "\n",
        "Here the process is as follows:-\n",
        "\n",
        "At each epoch,\n",
        "\n",
        "- documents are shuffled and sorted\n",
        "- stream is cut down into batches which consist of mini-batches of fixed size\n",
        "- and these are passed to the model.??????\n",
        "\n",
        "This is done by the `LMDataLoader` which creates dependent and independent variables for us but we need to pass it numericalized tokens **had difficulty in LMDataLoaders part**"
      ],
      "metadata": {
        "id": "88JTZIKDSqzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stream = \"In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the PreProcessor used in the data block API.\\nThen we will study how we build a language model and train it for a while.\"\n",
        "\n",
        "bs,seq_len=6,15\n",
        "tokens=tknr(stream)\n",
        "d_tokens=np.array([tokens[i*15:i*15+seq_len] for i in range(bs)])\n",
        "df=pd.DataFrame(d_tokens)\n",
        "\n",
        "# forgot this line of code\n",
        "display(HTML( df.to_html(index=False,header=None)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "iLzGapN2oSum",
        "outputId": "1d63683a-7653-480f-cc1c-2f0c8405b39c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>xxbos</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>in</td>\n",
              "      <td>this</td>\n",
              "      <td>chapter</td>\n",
              "      <td>,</td>\n",
              "      <td>we</td>\n",
              "      <td>will</td>\n",
              "      <td>go</td>\n",
              "      <td>back</td>\n",
              "      <td>over</td>\n",
              "      <td>the</td>\n",
              "      <td>example</td>\n",
              "      <td>of</td>\n",
              "      <td>classifying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>movie</td>\n",
              "      <td>reviews</td>\n",
              "      <td>we</td>\n",
              "      <td>studied</td>\n",
              "      <td>in</td>\n",
              "      <td>chapter</td>\n",
              "      <td>1</td>\n",
              "      <td>and</td>\n",
              "      <td>dig</td>\n",
              "      <td>deeper</td>\n",
              "      <td>under</td>\n",
              "      <td>the</td>\n",
              "      <td>surface</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>first</td>\n",
              "      <td>we</td>\n",
              "      <td>will</td>\n",
              "      <td>look</td>\n",
              "      <td>at</td>\n",
              "      <td>the</td>\n",
              "      <td>processing</td>\n",
              "      <td>steps</td>\n",
              "      <td>necessary</td>\n",
              "      <td>to</td>\n",
              "      <td>convert</td>\n",
              "      <td>text</td>\n",
              "      <td>into</td>\n",
              "      <td>numbers</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>how</td>\n",
              "      <td>to</td>\n",
              "      <td>customize</td>\n",
              "      <td>it</td>\n",
              "      <td>.</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>by</td>\n",
              "      <td>doing</td>\n",
              "      <td>this</td>\n",
              "      <td>,</td>\n",
              "      <td>we</td>\n",
              "      <td>'ll</td>\n",
              "      <td>have</td>\n",
              "      <td>another</td>\n",
              "      <td>example</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>of</td>\n",
              "      <td>the</td>\n",
              "      <td>preprocessor</td>\n",
              "      <td>used</td>\n",
              "      <td>in</td>\n",
              "      <td>the</td>\n",
              "      <td>data</td>\n",
              "      <td>block</td>\n",
              "      <td>xxup</td>\n",
              "      <td>api</td>\n",
              "      <td>.</td>\n",
              "      <td>\\n</td>\n",
              "      <td>xxmaj</td>\n",
              "      <td>then</td>\n",
              "      <td>we</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>will</td>\n",
              "      <td>study</td>\n",
              "      <td>how</td>\n",
              "      <td>we</td>\n",
              "      <td>build</td>\n",
              "      <td>a</td>\n",
              "      <td>language</td>\n",
              "      <td>model</td>\n",
              "      <td>and</td>\n",
              "      <td>train</td>\n",
              "      <td>it</td>\n",
              "      <td>for</td>\n",
              "      <td>a</td>\n",
              "      <td>while</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "toks200=txts[:200].map(tknr)"
      ],
      "metadata": {
        "id": "pyyHVVrbr671"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nums200=toks200.map(num)"
      ],
      "metadata": {
        "id": "e2iPuHBmtgRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dl=LMDataLoader(nums200)"
      ],
      "metadata": {
        "id": "b-sy5qbwtkdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x,y=first(dl)"
      ],
      "metadata": {
        "id": "90HiTO_gtnz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHlwBYcvuXT2",
        "outputId": "d02f2a5b-abb3-41ae-c9f4-5a886abe0677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LMTensorText([[ 2,  0, 43,  ..., 30, 12,  0],\n",
              "              [ 0,  0, 51,  ...,  0,  0,  0],\n",
              "              [ 0, 30,  0,  ...,  8,  0,  8],\n",
              "              ...,\n",
              "              [ 0,  0,  0,  ...,  0,  0,  0],\n",
              "              [ 0,  0,  0,  ..., 32,  0,  0],\n",
              "              [ 0,  0,  0,  ...,  0,  0,  0]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking dependent variables\n",
        "' '.join(num.vocab[o] for o in x[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "BCTXP-NRtpqx",
        "outputId": "c2229f96-37f1-44ec-a949-e85e8f0ef0db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'xxbos xxunk - xxunk - xxunk xxunk xxunk xxunk xxmaj xxunk xxmaj xxunk xxunk xxunk xxunk xxunk xxunk xxunk xxunk - xxunk - xxunk xxunk xxunk xxunk xxunk xxunk a xxunk xxunk xxunk xxunk xxunk a xxunk xxunk xxunk xxunk xxunk xxunk xxunk xxunk xxunk xxunk xxunk . xxmaj xxunk xxunk xxunk xxunk xxunk xxunk xxunk xxunk xxunk xxunk - xxunk xxunk xxunk xxunk xxunk xxunk xxunk xxunk xxunk . a xxunk'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training text classifier\n",
        "\n",
        "1. language model using datablock\n",
        "2. fine tune it\n",
        "3. save and load\n",
        "4. text generation\n",
        "5. creating classifier dataloaders\n",
        "6. fine tuning it"
      ],
      "metadata": {
        "id": "dZFu2iIBusFt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "forgot these\n",
        "```\n",
        "batch_size=128,seq_len=80\n",
        "\n",
        "splitter=GrandparentSplitter(0.1) the numebr 0.1\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "LlPAoEf4xjcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_imdb=partial(get_text_files,folders=['train','test','unsup'])\n",
        "lm_dl=DataBlock(\n",
        "    blocks=TextBlock.from_folder(path,is_lm=True),\n",
        "                get_items=get_imdb,\n",
        "                #don;t need since lm has embedded labels in independent variable\n",
        "                #get_y=parent_label,\n",
        "                splitter=GrandparentSplitter(0.1)).dataloaders(path,path=path,batch_size=128,seq_len=80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "O4PK5fTZt-qg",
        "outputId": "f0a3b39c-648c-44c5-fe85-7f95fc963371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/root/.fastai/data/imdb_tok/counter.pkl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-2690a06141e1>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_imdb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_text_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfolders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'unsup'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m lm_dl=DataBlock(\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mblocks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTextBlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_lm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m                 \u001b[0mget_items\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_imdb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0;31m#don;t need since lm has embedded labels in independent variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/text/data.py\u001b[0m in \u001b[0;36mfrom_folder\u001b[0;34m(cls, path, vocab, is_lm, seq_len, backwards, min_freq, max_vocab, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_lm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m72\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackwards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;34m\"Build a `TextBlock` from a `path`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         return cls(Tokenizer.from_folder(path, **kwargs), vocab=vocab, is_lm=is_lm, seq_len=seq_len,\n\u001b[0m\u001b[1;32m    243\u001b[0m                    backwards=backwards, min_freq=min_freq, max_vocab=max_vocab)\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/text/core.py\u001b[0m in \u001b[0;36mfrom_folder\u001b[0;34m(cls, path, tok, rules, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrules\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrules\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         res = cls(tok, counter=load_pickle(output_dir/fn_counter_pkl),\n\u001b[0m\u001b[1;32m    283\u001b[0m                   lengths=load_pickle(output_dir/fn_lengths_pkl), rules=rules, mode='folder')\n\u001b[1;32m    284\u001b[0m         \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastcore/xtras.py\u001b[0m in \u001b[0;36mload_pickle\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;34m\"Load a pickle file from a file name or opened file\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;31m# %% ../nbs/03_xtras.ipynb 59\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastcore/xtras.py\u001b[0m in \u001b[0;36mopen_file\u001b[0;34m(fn, mode, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'.gz'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'.zip'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;31m# %% ../nbs/03_xtras.ipynb 56\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/root/.fastai/data/imdb_tok/counter.pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "drop_mult=0.3\n",
        "\n",
        ".to_fp16()\n",
        "```\n",
        "forgot these two\n"
      ],
      "metadata": {
        "id": "JHOz-PsGxYHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now pass this dataloader of language model to language model learner\n",
        "lm_learner=language_model_learner(lm_dl,AWD_LSTM,drop_mult=0.3,metrics=[accuracy,Perplexity()]).to_fp16()"
      ],
      "metadata": {
        "id": "NBbkSjuBwDz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fine tune it\n",
        "lm_learner.fit_one_cycle(1,2e-2)"
      ],
      "metadata": {
        "id": "g9zfTJdex1gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save and load learner"
      ],
      "metadata": {
        "id": "rRplhhixx_G8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save the 1 epoch learner\n",
        "lm_learner.save('1epoch')"
      ],
      "metadata": {
        "id": "9kkR4zx9yCQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load it\n",
        "learn=lm_learner('1epoch')"
      ],
      "metadata": {
        "id": "b-gQdCzLybu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now unfreeze the model and fine tune it\n",
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(10,2e-3)"
      ],
      "metadata": {
        "id": "ss-3FGlHy05_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now save the encoder i.e. the body and leave the task specific head"
      ],
      "metadata": {
        "id": "xpnCzXbpzKo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learn.save_encoder('fine_tuned')"
      ],
      "metadata": {
        "id": "fP7LlqL-zQt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text generation\n",
        "\n",
        "FORGOT what parameters predict takes"
      ],
      "metadata": {
        "id": "oR8bzz60zW5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT = \"I liked this movie because\"\n",
        "N=22\n",
        "N_SEN=2\n",
        "preds=[learn.predict(TEXT, N, temperature=0.7) for _ in range(N_SEN)]"
      ],
      "metadata": {
        "id": "f4g9n-DRzYw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating classifer dataloaders\n",
        "\n",
        "using DAtaBlock API\n",
        "\n",
        "forgot to consider vocab of dataloader of language model and not learner"
      ],
      "metadata": {
        "id": "03tK0-0L0EG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clas_dl=DataBlock(\n",
        "    blocks=(TextBlock.from_folder(path,vocab=lm_dl.vocab),CategoryBlock),\n",
        "    get_y=parent_label,\n",
        "    get_items=partial(get_text_files,folders=['train','test','unsup']),\n",
        "    splitter=GrandparentSplitter(valid_name='test')\n",
        ").dataloaders(path,path=path,batch_size=128,seq_len=72)"
      ],
      "metadata": {
        "id": "2La-YeSO0Huh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clas_dl.show_batch(max_n=2)"
      ],
      "metadata": {
        "id": "MGkSfLyA1Zyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### create learner\n",
        "\n",
        "forgot to_fp16"
      ],
      "metadata": {
        "id": "21Tlp7k41eVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learn=text_classifier_learner(clas_dl,AWD_LSTM, metrics=[accuracy],drop_mult=0.5).to_fp16()"
      ],
      "metadata": {
        "id": "FS_1q9Mf1nnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have to load thye encoder we saved after fine tuning language model on IMDb corpus(forgot this part"
      ],
      "metadata": {
        "id": "5K3FqbBX1_sJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learn.load_encoder('fine_tuned')"
      ],
      "metadata": {
        "id": "zt3Xxo7h2LGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What this does is that it the encoder part of the learn(text_classifier_learner) is replaced with that of language model encoder."
      ],
      "metadata": {
        "id": "PMSJc29Q2Pch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine tuning classifier\n",
        "\n",
        "first we fine tune once using fit_one_cycle\n",
        "\n",
        "then freeze all layers excpet the last 2 layers and freeze all excpet last three layers and then at last unfreeeze all layers"
      ],
      "metadata": {
        "id": "RY6m-zoS2mS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learn.fit_one_cycle(1,2e-2)"
      ],
      "metadata": {
        "id": "xPERzodc2lW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn.freeze_to(-2)\n",
        "learn.fit_one_cycle(1,slice(1e-2/(2.6**4),1e-2))"
      ],
      "metadata": {
        "id": "MbDfgZjV3Pui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn.freeze_to(-3)\n",
        "learn.fit_one_cycle(1,slice(5e-2/(2.6**4),5e-2))"
      ],
      "metadata": {
        "id": "2UcjKTAd3fqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unfreeze whole model\n",
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))"
      ],
      "metadata": {
        "id": "RizP-IUS3uFk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}